Our experiments in the hockey environment show that reinforcement learning agents can learn effective state-action strategies for offensive and defensive two player game dynamics. While our agents demonstrated promising training dynamics and signs of convergence when given sufficient training time, certain issues, such as errors in mirroring the opponent’s state space during self-play limited performance of our agents during the competition. Nonetheless, our agents successfully learned the necessary state–action pairs to beat the basic opponents, and the overall results indicate that further training and exposure to a broader range of gameplay situations could yield even better performance. We would expect that the smoothed total reward would converge close to 10 for both the weak and strong Basic-Opponent\\\\
Throughout this project, we observed that an agent's success is strongly tied to the diversity of experiences it encounters during training. In particular, training against a range of opponents, varying from weaker to stronger bots, ensures that the agent does not overfit to a single playstyle but instead develops a robust and adaptable policy. For future experiments, we would suggest a training configuration where opponents actions are randomly sampled from a diverse categorical distribution containing a wide range of agents with different playstyles. This set should include both weak and strong Basic-Opponent, but also self play and inter agent play when multiple different pre-trained agents based on our implemented algorithms are available.\\
In summary, this project highlights the importance of diverse experience sampling and careful training process design in reinforcement learning.
