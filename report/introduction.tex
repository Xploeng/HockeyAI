This project dives into the challenge of building and competing against reinforcement learning agents, pushing them to perform in dynamic environments. In this final report, we break down our agent implementations and share the key lessons learned in using machine learning for action planning.\\
The target reinforcement learning environment for this project is a simple laser-hockey gym environment published by the martius-lab \cite{hockey_GitHub}. It is a continuous control task where an agent competes against an opponent to score goals. The state space is 18-dimensional, representing positions, velocities, and game-specific features such as puck possession. The action space is 8-dimensional, with 4 controls each for the agent and opponent, governing movement and shooting. The environment requires strategic decision-making and precise control, making it a challenging benchmark for reinforcement learning for both off-model and model based agents. For early benchmarking and development of our agents, we use the Pendulum-v0 environment as part of the gymnasium library \cite{towers2024gymnasiumstandardinterfacereinforcement}.

\paragraph{Contributions}
The entire team worked together to create a flexible reinforcement learning pipeline for easy training and evaluation for multiple different agents and algorithms. This includes flexible training with fine tuning of pretrained agents and loading their checkpoints, evaluations with visual feedback of actual game dynamics and reward visualizations, and flexible self play and inter agent play options. Moreover we allowed for easy training statistics on the fly, monitoring losses like the actor, critic and alpha losses during SAC training to allow for easier hyperparameter adjustions using more knowledge about the underlying dynamics, quickly spotting unstable behaviour. The codebase is available at \hyperlink{https://github.com/Xploeng/HockeyAI}{https://github.com/Xploeng/HockeyAI}.\\\\
Julius Rau implemented Rainbow and NAF, Timo LÃ¼bbing implemented DDPG with Pinknoise and SAC, and Eric Nazarenius implemented TDMPC with behavioral cloning.